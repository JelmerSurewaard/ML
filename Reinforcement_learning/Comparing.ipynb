{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML 4 Mini project Snake\n",
    "---\n",
    "Begonnen met onderzoeken van Snake met SB3 door verschillende reward functies te testen met DQN.\n",
    "Dit leverde erg matige resultaten op.\n",
    "\n",
    "Reward functies die we allemaal hebben getest\n",
    "1.  +reward voor iedere stap die de snake zet  \n",
    "\n",
    "2.  +reward voor ieder stap die de snake zet  \n",
    "    -reward bij dood gaan  \n",
    "\n",
    "3.  +reward voor het eten van snoepje  \n",
    "    -reward bij dood gaan  \n",
    "    geen reward bij een stap zonder het eten van snoepje of dood gaan  \n",
    "\n",
    "4.  +reward voor het eten van snoepje  \n",
    "    -reward bij dood gaan  \n",
    "    -reward van afstand tot de appel bij iedere stap  \n",
    "\n",
    "Reward functie 4 gaf het beste resultaat in SB3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def food_reward(event, step_amount, apple_count, coord):\n",
    "    \n",
    "    if event == 1:\n",
    "        return 250\n",
    "    if event == -1:\n",
    "        print(\"I ate \" + str(apple_count) + \" apples this game\")\n",
    "        return -1000\n",
    "\n",
    "    snake_head = np.where(coord == 3)\n",
    "    fruit_pos = np.where(coord == 1)\n",
    "    \n",
    "    if snake_head[0].size != 0:\n",
    "        pos_x = snake_head[0][0]\n",
    "        pos_y = snake_head[1][0]\n",
    "        if fruit_pos[0].size != 0:\n",
    "            fruit_x = fruit_pos[0][0]\n",
    "            fruit_y = fruit_pos[1][0]\n",
    "\n",
    "            dis = abs(fruit_x - pos_x) + abs(fruit_y - pos_y)\n",
    "\n",
    "            #print(\"dis: \" + str(dis))\n",
    "            \n",
    "            #print(\"result: \" + str(result))\n",
    "            return -dis\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SB3 Snake](images\\SB3_Snake.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierna zijn we overgestapt na SB2 naar het advies van Erco  \n",
    "Met SB2 kregen we een heel stuk betere resultaten  \n",
    "\n",
    "We hebben verschillende tests gedaan met DQN waarbij we bijvoorbeeld hebben gespeeld met de exploration rate.  \n",
    "Hieruit bleek dat DQN veel moeite had om goede resultaten te behalen, de agent kreeg namelijk vooral moeite met het spel spelen wanneer de snake 3 tot 5 snoepjes op at omdat deze dan lang werd. Hier hadden agents veel moeite mee en leken vaak over te gaan naar semi random acties waardoor ze vaak dood gingen. Dit was te verklaren doordat de agent tijdens het trainen niet veel had getrained met lange snakes.  \n",
    "Door dit toe te voegen leek hij wel wat beter te worden maar kwam hij vaak niet verder dan het eten van 5/6 snoepjes op een grid van 6x6.  \n",
    "\n",
    "Wat wel opviel bij het trainen is dat het opnieuw trainen van modellen erg veel bijdraagt bij het leren. Hierdoor maakte de agent bijna iedere keer dat hij opnieuw werdt getraind een soort doorbraak en was hij aanzienlijk slimmer dan de vorige tot een bepaald niveau\n",
    "\n",
    "\n",
    "Hieronder zijn de tensorboard logs te zien van de episode reward van een DQN model wat is getrained.  \n",
    "Dit model is getrained over 200000 stappen waarbij\n",
    "- De oranje lijn is een model waarbij er 3 snoepjes aanwezig waren\n",
    "- De blauwe lijn was er 1 snoepje aanwezig\n",
    "- De groene lijn was er ook maar 1 snoepje aanwezig maar gaat het model naar een 0% exploration rate  \n",
    "\n",
    "![DQN 200k steps](images\\DQN_training_200k_steps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na deze onderzoeken naar DQN zijn we overgestapt naar PPO  \n",
    "Met PPO zagen we een stuk betere resultaten\n",
    "\n",
    "Dit plaatje hieronder is een logging van een PPO model wat 3x getrained is met 500000 stappen op een 6x6 grid  \n",
    "Hierbij is te zien dat hij iedere keer een stuk beter werd\n",
    "\n",
    "![PPO 500k steps 3x](images\\PPO_3x_500k.png)\n",
    "\n",
    "Hierna is er aan het PPO model extra parameters toegevoegd waarmee we de architectuur van het netwerk erachter groter hebben gemaakt en hebben we een schedular gebruikt voor het aanpassen van de learning rate.   \n",
    "Hiermee kregen we de volgende resultaten\n",
    "\n",
    "![PPO 500k steps 3x](images\\PPO_7x_500k.png)\n",
    "\n",
    "Hierbij is duidelijk te zien dat hij iedere keer een stuk beter wordt tot een reward van 10, in deze spellen eet hij 11 snoepjes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierna hebben we een snake getraind waarvan hieronder een gif te zien is.  \n",
    "Deze is 10x door getrained waarbij de eerste 5x de hoeveelheid snoepjes werd verminderd tot uiteindelijk 1 snoepje in het spel zat.  \n",
    "Bij de tweede 5x is de begin lengte van de snake opgehoogd waardoor hij daar ook al ervaring mee opdeed.  \n",
    "Dit had uiteindelijk niet heel veel verschil tenopzichte van de bovenstaande snake\n",
    "\n",
    "![SB2 Snake PPO](images/SB2_Snake_PPO.gif)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1a2bbe61f6a80771386be97c88758285231adb5de96b3acd623cf790785d5a7"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('StableBaselines2': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
